{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZkZn8xtDJet"
      },
      "outputs": [],
      "source": [
        "!python --version\n",
        "!pip install evaluate\n",
        "!pip install rdflib\n",
        "!pip install rouge_score\n",
        "import torch\n",
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict, load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    EncoderDecoderModel,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    DataCollatorForTokenClassification\n",
        ")\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch # Import torch to move models and data to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Define device here as well for clarity within this cell"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/omerday/nlp-idiom-he.git idiomem\n",
        "!git clone https://github.com/dice-group/LIdioms.git LIdioms"
      ],
      "metadata": {
        "id": "4c9RKeUQDNZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import rdflib\n",
        "\n",
        "LIDIOM_LANGUAGES = [('english', 'en'), ('german', 'de'), ('italian', 'it'), ('portuguese', 'pt'), ('russian', 'rus')]\n",
        "\n",
        "def parse_lidioms():\n",
        "  translation_dataset = {}\n",
        "  for language, prefix in LIDIOM_LANGUAGES:\n",
        "    translation_dataset[language] = []\n",
        "    g = rdflib.Graph()\n",
        "    try:\n",
        "        g.parse(f\"LIdioms/{prefix}/{language}.ttl\", format=\"turtle\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing file: {e}\")\n",
        "        return []\n",
        "\n",
        "    # SPARQL query to find the label and definition pairs by traversing the graph\n",
        "    query = \"\"\"\n",
        "        PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
        "        PREFIX ontolex: <http://www.w3.org/ns/lemon/ontolex#>\n",
        "        PREFIX skos: <http://www.w3.org/2004/02/skos/core#>\n",
        "\n",
        "        SELECT ?label ?definition\n",
        "        WHERE {\n",
        "          ?entry a ontolex:LexicalEntry ;\n",
        "                 rdfs:label ?label ;\n",
        "                 ontolex:sense ?sense .\n",
        "          ?sense ontolex:isLexicalizedSenseOf ?concept .\n",
        "          ?concept skos:definition ?definition .\n",
        "        }\n",
        "    \"\"\"\n",
        "\n",
        "    results = g.query(query)\n",
        "\n",
        "    for row in results:\n",
        "        # The query returns Literal objects, we convert them to strings\n",
        "        if not row.label or not row.definition:\n",
        "            continue\n",
        "        translation_dataset[language].append(f'The English definition of {row.label} is: {row.definition}')\n",
        "\n",
        "    print(f\"{language.upper()} Corpus: {len(translation_dataset[language])}\")\n",
        "\n",
        "  return translation_dataset\n",
        "\n",
        "translation_dataset = parse_lidioms()"
      ],
      "metadata": {
        "id": "O7-Ad91HDQQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_hebrew_translation():\n",
        "  hebrew_sentences = []\n",
        "  with open(\"idiomem/hebrew_idioms_with_english_translation.txt\", \"r\") as file:\n",
        "    for line in file:\n",
        "      hebrew_sentences.append(line.replace(\"\\n\", \"\"))\n",
        "  return hebrew_sentences\n"
      ],
      "metadata": {
        "id": "kYOYigDVDWRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_CHECKPOINTS = {\n",
        "    \"M2M100\": \"facebook/m2m100_418M\",\n",
        "    \"mBART\": \"facebook/mbart-large-50\",\n",
        "    \"mT5\": \"google/mt5-base\",\n",
        "    \"mBERT\": \"bert-base-multilingual-cased\",\n",
        "    \"XLM-RoBERTa\": \"xlm-roberta-base\",\n",
        "}"
      ],
      "metadata": {
        "id": "NvVxQR1qDnE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples, tokenizer, max_input_length=128, max_target_length=128):\n",
        "    \"\"\"Tokenizes the input and target texts for a seq2seq task.\"\"\"\n",
        "    dropped = 0\n",
        "    inputs = []\n",
        "    targets = []\n",
        "    for ex in examples[\"text\"]:\n",
        "      if \" is: \" in ex:\n",
        "        inputs.append(ex.split(\" is: \")[0] + \" is: \")\n",
        "        targets.append(ex.split(\" is: \")[1])\n",
        "      else:\n",
        "        dropped += 1\n",
        "        inputs.append(\"\")\n",
        "        targets.append(\"\")\n",
        "\n",
        "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
        "    labels = tokenizer(text_target=targets, max_length=max_target_length, truncation=True)\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    print(f\"Dropped sentences: {dropped}\")\n",
        "    return model_inputs\n",
        "\n",
        "# Load the ROUGE metric for evaluation\n",
        "rouge = evaluate.load(\"rouge\")"
      ],
      "metadata": {
        "id": "9SuxwmKeDqfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred, tokenizer):\n",
        "    \"\"\"Computes ROUGE, token-level F1/Precision/Recall, and strict accuracy.\"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # ROUGE score\n",
        "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "\n",
        "    # Calculate token-level F1, Precision, Recall\n",
        "    total_f1, total_precision, total_recall = 0, 0, 0\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for pred, label in zip(decoded_preds, decoded_labels):\n",
        "        pred_tokens = set(pred.strip().split())\n",
        "        label_tokens = set(label.strip().split())\n",
        "\n",
        "        intersection = len(pred_tokens.intersection(label_tokens))\n",
        "\n",
        "        if len(pred_tokens) > 0:\n",
        "            precision = intersection / len(pred_tokens)\n",
        "        else:\n",
        "            precision = 0\n",
        "\n",
        "        if len(label_tokens) > 0:\n",
        "            recall = intersection / len(label_tokens)\n",
        "        else:\n",
        "            recall = 0\n",
        "\n",
        "        if (precision + recall) > 0:\n",
        "            f1 = 2 * (precision * recall) / (precision + recall)\n",
        "        else:\n",
        "            f1 = 0\n",
        "\n",
        "        total_precision += precision\n",
        "        total_recall += recall\n",
        "        total_f1 += f1\n",
        "\n",
        "        # Calculate strict accuracy (exact match)\n",
        "        if pred.strip() == label.strip():\n",
        "            correct_predictions += 1\n",
        "\n",
        "    num_samples = len(decoded_labels)\n",
        "    result['f1'] = (total_f1 / num_samples) * 100\n",
        "    result['precision'] = (total_precision / num_samples) * 100\n",
        "    result['recall'] = (total_recall / num_samples) * 100\n",
        "    result['accuracy'] = (correct_predictions / num_samples) * 100\n",
        "\n",
        "    return {k: round(v, 4) for k, v in result.items()}"
      ],
      "metadata": {
        "id": "PvS6MziiDuII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_translation_examples(trainer, tokenizer, test_dataset, num_examples=3):\n",
        "    \"\"\"Predicts and prints a few translation examples.\"\"\"\n",
        "    print(\"\\n--- Translation Examples ---\")\n",
        "\n",
        "    generation_kwargs = {\n",
        "        \"min_length\": 5,\n",
        "        \"max_length\": 128,\n",
        "        \"num_beams\": 4, # Beam search can find better sequences\n",
        "    }\n",
        "\n",
        "    # Get predictions\n",
        "    predictions = trainer.predict(test_dataset, **generation_kwargs)\n",
        "    preds = predictions.predictions\n",
        "    labels = predictions.label_ids\n",
        "\n",
        "    # Decode\n",
        "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    decoded_preds = [pred.replace(\"<extra_id_0>\", \"\").strip() for pred in decoded_preds]\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Get original inputs\n",
        "    inputs = tokenizer.batch_decode(test_dataset['input_ids'], skip_special_tokens=True)\n",
        "\n",
        "    for i in range(min(num_examples, len(inputs))):\n",
        "        print(f\"INPUT:    {inputs[i]}\")\n",
        "        print(f\"EXPECTED: {decoded_labels[i]}\")\n",
        "        print(f\"PREDICTED:  {decoded_preds[i]}\\n\")\n",
        "        print(\"-\" * 25)"
      ],
      "metadata": {
        "id": "tOHh-uY-DwZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(model_name, model_checkpoint, training_data, test_data, src_lang, tgt_lang):\n",
        "    \"\"\"Fine-tunes and evaluates a model, now with example printing.\"\"\"\n",
        "    print(f\"\\n--- Running experiment for {model_name} ---\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "    if \"mbart\" in model_checkpoint.lower():\n",
        "        tokenizer.src_lang = src_lang\n",
        "        tokenizer.tgt_lang = tgt_lang\n",
        "    elif \"m2m100\" in model_checkpoint.lower():\n",
        "        tokenizer.src_lang = src_lang[:2]\n",
        "        tokenizer.tgt_lang = tgt_lang[:2]\n",
        "\n",
        "    raw_datasets = DatasetDict({\n",
        "        'train': Dataset.from_dict({'text': training_data}),\n",
        "        'test': Dataset.from_dict({'text': test_data})\n",
        "    })\n",
        "    tokenized_datasets = raw_datasets.map(\n",
        "        lambda x: preprocess_function(x, tokenizer), batched=True\n",
        "    )\n",
        "\n",
        "    if \"bert\" in model_checkpoint or \"roberta\" in model_checkpoint:\n",
        "        print(f\"Adapting {model_name} for Seq2Seq task using EncoderDecoderModel.\")\n",
        "        model = EncoderDecoderModel.from_encoder_decoder_pretrained(\n",
        "            model_checkpoint, model_checkpoint\n",
        "        )\n",
        "        model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
        "        model.config.eos_token_id = tokenizer.sep_token_id\n",
        "        model.config.pad_token_id = tokenizer.pad_token_id\n",
        "        model.generation_config.decoder_start_token_id = tokenizer.cls_token_id\n",
        "        model.generation_config.eos_token_id = tokenizer.sep_token_id\n",
        "        model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
        "    else:\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
        "\n",
        "    args = Seq2SeqTrainingArguments(\n",
        "        output_dir=f\"./results/{model_name}_finetuned\",\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=4,\n",
        "        per_device_eval_batch_size=4,\n",
        "        weight_decay=0.01,\n",
        "        save_total_limit=2,\n",
        "        num_train_epochs=10,\n",
        "        predict_with_generate=True,\n",
        "        fp16=False,\n",
        "        logging_steps=10,\n",
        "        generation_max_length=128\n",
        "    )\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "    trainer = Seq2SeqTrainer(\n",
        "        model, args,\n",
        "        train_dataset=tokenized_datasets[\"train\"],\n",
        "        eval_dataset=tokenized_datasets[\"test\"],\n",
        "        data_collator=data_collator,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=lambda p: compute_metrics(p, tokenizer),\n",
        "    )\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    trainer.train()\n",
        "    print(\"Evaluating...\")\n",
        "    eval_results = trainer.evaluate()\n",
        "\n",
        "    show_translation_examples(trainer, tokenizer, tokenized_datasets[\"test\"], 10)\n",
        "\n",
        "    return eval_results"
      ],
      "metadata": {
        "id": "_-a3YbglDze_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idiom_data = parse_lidioms()\n",
        "idiom_data[\"hebrew\"] = parse_hebrew_translation()\n",
        "hebrew_dataset = idiom_data[\"hebrew\"]\n",
        "hebrew_train_test_split = Dataset.from_dict({'text': hebrew_dataset}).train_test_split(test_size=0.5, seed=42)\n",
        "hebrew_train = hebrew_train_test_split['train']['text']\n",
        "hebrew_test = hebrew_train_test_split['test']['text']\n",
        "\n",
        "print(f\"Hebrew training dataset is of size {len(hebrew_train)} and includes, for example:\\n{hebrew_train[:5]}\")\n",
        "print(f\"Hebrew testing dataset is of size {len(hebrew_test)} and includes, for example:\\n{hebrew_test[:5]}\")\n",
        "\n",
        "all_languages_train = [idiom for lang, idioms in idiom_data.items() if lang != \"hebrew\" for idiom in idioms]\n",
        "print(f\"ALL_LANGUAGE training dataset size: {len(all_languages_train)}\")\n",
        "print(f\"ENGLISH training dataset size: {len(idiom_data['english'])}\")\n",
        "print(f\"HEBREW training dataset size: {len(hebrew_train)}\")\n",
        "\n",
        "experiments = {\n",
        "    \"Hebrew-Only\": {\"train\": hebrew_train, \"test\": hebrew_test, \"src\": \"he_IL\", \"tgt\": \"en_XX\"},\n",
        "    \"English-Only (Zero-Shot)\": {\"train\": idiom_data[\"english\"], \"test\": hebrew_dataset, \"src\": \"en_XX\", \"tgt\": \"en_XX\"},\n",
        "    \"All-Languages (Zero-Shot)\": {\"train\": all_languages_train, \"test\": hebrew_dataset, \"src\": \"en_XX\", \"tgt\": \"en_XX\"}\n",
        "}\n",
        "\n",
        "final_results = {}\n",
        "for model_name, model_checkpoint in MODEL_CHECKPOINTS.items():\n",
        "    final_results[model_name] = {}\n",
        "    for exp_name, data_config in experiments.items():\n",
        "        print(f\"\\n{'='*25}\\nMODEL: {model_name} | EXPERIMENT: {exp_name}\\n{'='*25}\")\n",
        "        results = run_experiment(\n",
        "            model_name,\n",
        "            model_checkpoint,\n",
        "            data_config[\"train\"],\n",
        "            data_config[\"test\"],\n",
        "            data_config[\"src\"],\n",
        "            data_config[\"tgt\"]\n",
        "        )\n",
        "        # Store key metrics for summary table\n",
        "        final_results[model_name][exp_name] = {\n",
        "            'ROUGE-1': results.get('eval_rouge1', 0),\n",
        "            'F1-Score': results.get('eval_f1', 0),\n",
        "            'Accuracy': results.get('eval_accuracy', 0)\n",
        "        }\n",
        "\n",
        "print(\"\\n\\n--- Experiment Summary ---\")\n",
        "# Reformat results for better display with multi-level columns\n",
        "reformated_results = {}\n",
        "for model, exps in final_results.items():\n",
        "    for exp_name, metrics in exps.items():\n",
        "        if exp_name not in reformated_results:\n",
        "            reformated_results[exp_name] = {}\n",
        "        reformated_results[exp_name][model] = metrics\n",
        "\n",
        "# Print a separate table for each experiment scenario\n",
        "for exp_name, model_metrics in reformated_results.items():\n",
        "    print(f\"\\n--- SCENARIO: {exp_name} ---\\n\")\n",
        "    df = pd.DataFrame(model_metrics).T # Transpose to have models as rows\n",
        "    print(df.to_markdown())"
      ],
      "metadata": {
        "id": "6wBCrF4QD5KN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nrYjLL0XdeFX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}